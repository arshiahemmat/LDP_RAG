{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "API_KEY = userdata.get('avalai_api')"
      ],
      "metadata": {
        "id": "XGu05kBUAIcu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai python-docx langchain-community langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxmVp5vTAQld",
        "outputId": "0578d742-643c-42fd-fa00-cc63e4f2b4f2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q json5"
      ],
      "metadata": {
        "id": "4cqlIccjxwRA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"14ub55iNBBbZyedB7C1U6y-tC1S_1RRcf\"\n",
        "!unzip Data.zip"
      ],
      "metadata": {
        "id": "F087Hq20SMll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptive Chunking"
      ],
      "metadata": {
        "id": "44o7Xy0KCa4P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "wipZP1YMABcr"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from docx import Document\n",
        "import re\n",
        "import json # Import the json module\n",
        "\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Initialize the Gemini model for classification and token counting\n",
        "CLASSIFICATION_MODEL_NAME = 'gemini-2.5-flash'\n",
        "classification_model = genai.GenerativeModel(CLASSIFICATION_MODEL_NAME)\n",
        "\n",
        "# Function to extract text from DOCX\n",
        "def extract_text_from_docx(docx_path):\n",
        "    \"\"\"Extracts all text from a DOCX file.\"\"\"\n",
        "    try:\n",
        "        document = Document(docx_path)\n",
        "        text = []\n",
        "        for paragraph in document.paragraphs:\n",
        "            text.append(paragraph.text)\n",
        "        return \"\\n\".join(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from DOCX: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Token counting function using Gemini's model\n",
        "def count_tokens(text):\n",
        "    \"\"\"Returns the number of tokens in the input text using Gemini's model.\"\"\"\n",
        "    try:\n",
        "        # Use the model's built-in token counting method\n",
        "        response = classification_model.count_tokens(text)\n",
        "        # print(f\"DEBUG: Token count for '{text[:50]}...' (API): {response.total_tokens}\")\n",
        "        return response.total_tokens\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Error counting tokens with Gemini API: {e}. Using fallback estimate.\")\n",
        "        # Fallback to a simple character-based estimate if API call fails\n",
        "        estimated_tokens = len(text) // 4 # Rough estimate: 1 token ~ 4 characters\n",
        "        # print(f\"DEBUG: Token count for '{text[:50]}...' (Fallback): {estimated_tokens}\")\n",
        "        return estimated_tokens\n",
        "\n",
        "# Function to chunk text while respecting token limit and sentence boundaries\n",
        "def chunk_text_by_tokens(text, max_tokens=500):\n",
        "    \"\"\"Chunks the text into chunks of sentences while ensuring each chunk doesn't exceed the max token limit.\"\"\"\n",
        "    # A more robust sentence split using regex to handle various punctuation and spaces\n",
        "    # Ensures that the delimiter (e.g., '.', '!', '?') is included at the end of the sentence\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    # Add back the delimiter if it was removed by split, or ensure it's there\n",
        "    processed_sentences = []\n",
        "    for s in sentences:\n",
        "        s_stripped = s.strip()\n",
        "        if s_stripped: # Ensure sentence is not empty\n",
        "            # If the sentence doesn't end with a common punctuation, add a period for consistency\n",
        "            if not re.search(r'[.!?]$', s_stripped):\n",
        "                s_stripped += '.'\n",
        "            processed_sentences.append(s_stripped)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk_sentences = []\n",
        "    current_token_count = 0\n",
        "    sentence_ids = []\n",
        "\n",
        "    for idx, sentence in enumerate(processed_sentences):\n",
        "        # Count tokens for the current sentence\n",
        "        sentence_token_count = count_tokens(sentence)\n",
        "\n",
        "        # print(f\"DEBUG: Processing sentence {idx+1}: '{sentence[:50]}...' (Tokens: {sentence_token_count})\")\n",
        "        # print(f\"DEBUG: Current chunk tokens: {current_token_count}, Max tokens: {max_tokens}\")\n",
        "\n",
        "        # If adding the sentence would exceed the limit, finalize the current chunk\n",
        "        # This condition ensures a new chunk is created if the current one is too large,\n",
        "        # or if the very first sentence itself is larger than max_tokens.\n",
        "        if current_token_count + sentence_token_count > max_tokens and current_chunk_sentences:\n",
        "            # Finalize the current chunk\n",
        "            chunks.append({\n",
        "                'chunk_id': len(chunks) + 1,\n",
        "                'tag': 'Public',  # Placeholder tag, will be updated later\n",
        "                'content': ' '.join(current_chunk_sentences).strip(),\n",
        "                'source_sentence_ids': sentence_ids\n",
        "            })\n",
        "            # print(f\"DEBUG: Chunk {len(chunks)} finalized. Starting new chunk.\")\n",
        "            # Start a new chunk with the current sentence\n",
        "            current_chunk_sentences = [sentence]\n",
        "            sentence_ids = [idx + 1]  # sentence IDs start from 1\n",
        "            current_token_count = sentence_token_count\n",
        "        elif sentence_token_count > max_tokens:\n",
        "            # Handle case where a single sentence is larger than max_tokens\n",
        "            # It will form its own chunk. This might lead to a chunk exceeding max_tokens,\n",
        "            # but it respects sentence boundaries as requested.\n",
        "            chunks.append({\n",
        "                'chunk_id': len(chunks) + 1,\n",
        "                'tag': 'Public',\n",
        "                'content': sentence,\n",
        "                'source_sentence_ids': [idx + 1]\n",
        "            })\n",
        "            print(f\"DEBUG: Sentence {idx+1} (Tokens: {sentence_token_count}) > Max tokens. Creating single-sentence chunk.\")\n",
        "            current_chunk_sentences = [] # Reset for next sentence\n",
        "            sentence_ids = []\n",
        "            current_token_count = 0\n",
        "        else:\n",
        "            # Add the sentence to the current chunk\n",
        "            current_chunk_sentences.append(sentence)\n",
        "            sentence_ids.append(idx + 1)\n",
        "            current_token_count += sentence_token_count\n",
        "\n",
        "        # print(f\"DEBUG: After processing sentence {idx+1}: Current chunk tokens: {current_token_count}\")\n",
        "\n",
        "    # Add the last chunk if it contains content\n",
        "    if current_chunk_sentences:\n",
        "        chunks.append({\n",
        "            'chunk_id': len(chunks) + 1,\n",
        "            'tag': 'Public',  # Placeholder tag, will be updated later\n",
        "            'content': ' '.join(current_chunk_sentences).strip(),\n",
        "            'source_sentence_ids': sentence_ids\n",
        "        })\n",
        "        # print(f\"DEBUG: Final chunk {len(chunks)} added.\")\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to classify each chunk using Gemini\n",
        "def classify_chunk(chunk_content):\n",
        "    \"\"\"Classifies a chunk using the Gemini API.\"\"\"\n",
        "    prompt = (\n",
        "        \"Classify the following text into one of these categories: 'Public', 'Sensitive', or 'Confidential'. \"\n",
        "        \"Provide only the category name as your response, without any additional text or explanation.\\n\\n\"\n",
        "        f\"Text: \\\"\\\"\\\"{chunk_content}\\\"\\\"\\\"\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = classification_model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                max_output_tokens=10, # Keep output short for classification\n",
        "                temperature=0.0,      # Aim for deterministic classification\n",
        "            )\n",
        "        )\n",
        "        # Extract the text from the response\n",
        "        if response.candidates:\n",
        "            # Access the text directly from the first part of the first candidate\n",
        "            tag = response.candidates[0].content.parts[0].text.strip()\n",
        "            # Basic validation to ensure the tag is one of the expected values\n",
        "            if tag in ['Public', 'Sensitive', 'Confidential']:\n",
        "                return tag\n",
        "            else:\n",
        "                print(f\"Warning: Unexpected classification result: '{tag}'. Defaulting to 'Public'.\")\n",
        "                return 'Public'\n",
        "        else:\n",
        "            print(\"Warning: No candidates found in Gemini API response. Defaulting to 'Public'.\")\n",
        "            return 'Public'\n",
        "    except Exception as e:\n",
        "        print(f\"Error classifying chunk with Gemini API: {e}. Defaulting to 'Public'.\")\n",
        "        return 'Public'\n",
        "\n",
        "# Function to save chunked data to a JSON file\n",
        "def save_chunks_to_json(chunks_data, output_filename=\"chunked_data.json\"):\n",
        "    \"\"\"Saves the list of chunk dictionaries to a JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(chunks_data, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"Chunked data successfully saved to {output_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving chunked data to JSON file: {e}\")\n",
        "\n",
        "# Main function to process DOCX, chunk the text, and tag the chunks\n",
        "def process_document(docx_path, output_filename, max_tokens=500):\n",
        "    \"\"\"Processes a DOCX file, chunking it and classifying each chunk.\"\"\"\n",
        "    # Step 1: Extract text from the DOCX file\n",
        "    text = extract_text_from_docx(docx_path)\n",
        "    if not text:\n",
        "        print(\"No text extracted from the document. Exiting.\")\n",
        "        return []\n",
        "\n",
        "    # Step 2: Chunk the text into manageable parts\n",
        "    chunks = chunk_text_by_tokens(text, max_tokens)\n",
        "\n",
        "    # Step 3: Classify each chunk and assign the appropriate tag\n",
        "    chunk_metadata = []\n",
        "    for chunk in chunks:\n",
        "        print(f\"Classifying chunk {chunk['chunk_id']}...\")\n",
        "        tag = classify_chunk(chunk['content'])\n",
        "        chunk['content'] = f'[{tag.upper()}]:' + chunk['content']\n",
        "        chunk['tag'] = tag  # Update the chunk's tag\n",
        "        chunk_metadata.append(chunk)\n",
        "\n",
        "    save_chunks_to_json(chunk_metadata,output_filename= output_filename)\n",
        "    # return chunk_metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_path = \"/content/Data/Docs/Customer Service/1.docx\"\n",
        "process_document(document_path, output_filename=\"chunked_data_tag.json\")\n",
        "# print(f\"-------ch: {chunk_metadata}------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "hXHEPDVQ5BQq",
        "outputId": "ca20ace8-7366-44d5-f874-69d814023135"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifying chunk 1...\n",
            "Error classifying chunk with Gemini API: list index out of range. Defaulting to 'Public'.\n",
            "Classifying chunk 2...\n",
            "Error classifying chunk with Gemini API: list index out of range. Defaulting to 'Public'.\n",
            "Classifying chunk 3...\n",
            "Error classifying chunk with Gemini API: list index out of range. Defaulting to 'Public'.\n",
            "Chunked data successfully saved to chunked_data_tag.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_path = \"/content/Data/Docs/Customer Service/1.docx\"\n",
        "doc = extract_text_from_docx(document_path)\n",
        "chunks = chunk_text_by_tokens(doc)\n",
        "print(chunks)"
      ],
      "metadata": {
        "id": "Jqi7TiEdSKks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdlTClRdSKiT",
        "outputId": "781c6114-5443-406b-df41-1820164ad9e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "CbLIvkvaSKfl",
        "outputId": "239a02ba-80c4-4b07-fcd6-a83ad6b4f335"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Understanding Customer Service Excellence: A Comprehensive Guide\\n\\n1. Introduction to Customer Service Excellence\\n Customer service excellence is the practice of providing outstanding support and service to customers at every point of contact. It involves understanding customer needs, exceeding their expectations, and building lasting relationships. Exceptional customer service can differentiate a business from its competitors and drive customer loyalty and retention. 2. Key Principles of Customer Service Excellence\\n The key principles of customer service excellence include empathy, responsiveness, reliability, and communication. Empathy ensures that customers feel heard and understood, while responsiveness demonstrates a commitment to meeting their needs. Reliability builds trust, and effective communication ensures that customers are always informed and engaged. 3. The Importance of Customer Satisfaction\\n Customer satisfaction is crucial to the success of any business. Satisfied customers are more likely to become repeat clients, recommend the business to others, and contribute to positive reviews. A focus on customer satisfaction leads to long-term loyalty and is often more cost-effective than acquiring new customers. 4. Effective Communication in Customer Service\\n Effective communication is one of the most important aspects of customer service excellence. It involves clear, concise, and polite communication with customers, ensuring that their needs and concerns are addressed appropriately. Active listening, asking the right questions, and providing solutions in a timely manner are key to successful communication in customer service. 5. Empathy and Emotional Intelligence in Customer Service\\n Empathy and emotional intelligence play a significant role in delivering excellent customer service. Understanding the emotions and perspectives of customers helps to resolve issues more effectively and ensures that customers feel valued. A customer service representative who can empathize with a frustrated customer can help de-escalate tense situations and provide more effective solutions. 6. Handling Customer Complaints\\n Customer complaints are inevitable, but they can be an opportunity to turn a dissatisfied customer into a loyal one. The key to handling complaints effectively is to listen attentively, apologize sincerely, and offer a solution that addresses the customer’s concerns. Prompt resolution of issues shows that the business values its customers and is willing to go the extra mile to meet their needs. 7. Training and Development in Customer Service\\n Ongoing training and development are essential for maintaining high standards of customer service. Staff should be trained on product knowledge, customer interaction, problem-solving, and conflict resolution. Continuous development helps customer service teams stay up-to-date with best practices and enables them to handle a variety of customer needs more effectively. 8.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Execute the process\n",
        "chunked_data = process_document(document_path, max_tokens=500)\n",
        "\n",
        "# Step 5: Save the chunked data to a JSON file\n",
        "if chunked_data:\n",
        "    save_chunks_to_json(chunked_data, output_filename='chunked_output.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "MX6Gs-kOSKc_",
        "outputId": "189b891d-bbeb-4e94-dea7-e97d4d146f09"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifying chunk 1...\n",
            "Error classifying chunk with Gemini API: list index out of range. Defaulting to 'Public'.\n",
            "Classifying chunk 2...\n",
            "Error classifying chunk with Gemini API: list index out of range. Defaulting to 'Public'.\n",
            "Classifying chunk 3...\n",
            "Error classifying chunk with Gemini API: list index out of range. Defaulting to 'Public'.\n",
            "Chunked data successfully saved to chunked_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pqu-RmMfSKYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HAuJ42ddSKWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2B85dDKQSKTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Rpd2InxSKN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N084xqhDSKLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Privacy Gateway"
      ],
      "metadata": {
        "id": "l9_IGoIiCeSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "# ─────────── 0.  MODEL HANDLES ──────────────────────────────\n",
        "def make_llm(temp: float) -> ChatOpenAI:\n",
        "    return ChatOpenAI(\n",
        "        base_url = \"https://api.avalai.ir/v1\",\n",
        "        model=\"gpt-4o\",\n",
        "        api_key=API_KEY,\n",
        "        temperature=temp,\n",
        "    )\n",
        "\n",
        "LLM_PRECISE = make_llm(0.0)   # Phase A\n",
        "LLM_DEEP    = make_llm(0.7)   # Phase B\n",
        "\n",
        "MAX_TOKENS = 512\n",
        "\n",
        "# ─────────── 1-A.  PRECISION PROMPT  (Phase A) ─────────────\n",
        "PRECISION_TMPL = \"\"\"\n",
        "You are the Privacy Gate.\n",
        "\n",
        "Rewrite the text so that its vagueness matches ε = {epsilon}.\n",
        "\n",
        "Guidelines\n",
        "• ε = 1.0  → keep almost all specifics; redact only obvious secrets\n",
        "• ε = 0.7  → generalise a little\n",
        "• ε = 0.5  → remove/blur most specifics\n",
        "• ε = 0.3  → keep only broad actions\n",
        "• ε = 0.1  → bare outline\n",
        "\n",
        "Do NOT remove the core meaning.\n",
        "\n",
        "Label: {label}\n",
        "Original text:\n",
        "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
        "\n",
        "Return JSON only:\n",
        "{{\"rewritten\": \"<sanitised>\"}}\"\"\"\n",
        "\n",
        "# ─────────── 1-B.  DEEP-OBFUSCATE PROMPT (Phase B) ─────────\n",
        "DEEP_TMPL = \"\"\"\n",
        "You are the Privacy Gate – deep-obfuscation mode.\n",
        "\n",
        "Take the input text and rewrite it so it is **even vaguer** than before:\n",
        "• shorten sentences,\n",
        "• replace any remaining specifics with generic terms,\n",
        "• rephrase with **different wording** than the previous version,\n",
        "• keep overall intent.\n",
        "\n",
        "Return JSON only:\n",
        "{{\"rewritten\": \"<even more vague text>\"}}\"\"\"\n",
        "\n",
        "# ─────────── 2.  LOW-LEVEL CALL HELPERS ─────────────────────\n",
        "_JSON_RE = re.compile(r\"\\{.*\\}\", re.DOTALL)\n",
        "\n",
        "def _call(llm: ChatOpenAI, prompt: str) -> str:\n",
        "    resp = llm(\n",
        "        [SystemMessage(content=\"You are a privacy-focused assistant.\"),\n",
        "         HumanMessage(content=prompt)],\n",
        "        max_tokens=MAX_TOKENS,\n",
        "    ).content.strip()\n",
        "    m = _JSON_RE.search(resp)\n",
        "    if not m:\n",
        "        raise ValueError(f\"Missing JSON:\\n{resp}\")\n",
        "    obj = json.loads(m.group(0))\n",
        "    if \"rewritten\" not in obj:\n",
        "        raise ValueError(f\"No 'rewritten' key:\\n{obj}\")\n",
        "    return obj[\"rewritten\"]\n",
        "\n",
        "# ─────────── 3-A.  single-pass sanitiser (Phase A) ─────────\n",
        "def sanitise_once(chunk: str, label: str, ε: float) -> str:\n",
        "    prompt = PRECISION_TMPL.format(\n",
        "        label=label, epsilon=ε, chunk=chunk.replace('\"', '\\\\\"')\n",
        "    )\n",
        "    return _call(LLM_PRECISE, prompt)\n",
        "\n",
        "# ─────────── 3-B.  deep-obfuscate step (Phase B) ───────────\n",
        "def deep_obfuscate(prev: str) -> str:\n",
        "    prompt = DEEP_TMPL.format(chunk=prev.replace('\"', '\\\\\"'))\n",
        "    return _call(LLM_DEEP, prompt)\n",
        "\n",
        "# ─────────── 4.  public pipeline ───────────────────────────\n",
        "def privacy_gate_pipeline(\n",
        "    text: str,\n",
        "    label: Literal[\"PUBLIC\", \"SENSITIVE\", \"CONFIDENTIAL\"],\n",
        "    eps_schedule: List[float] = (1.0, 0.7, 0.5, 0.3, 0.1),\n",
        "    deep_rounds: int = 4,\n",
        ") -> Dict[float, List[str]]:\n",
        "    \"\"\"\n",
        "    Phase A: run once for every ε in eps_schedule\n",
        "    Phase B: take ε=min(eps_schedule) output, run 'deep_rounds' extra passes\n",
        "    Returns {ε: [v1, v2, …]}  (ε>min → one element; ε_min → deep_rounds+1 elems)\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    current = text\n",
        "    for ε in eps_schedule:\n",
        "        current = sanitise_once(current, label, ε)\n",
        "        results[ε] = [current]\n",
        "\n",
        "    ε_min = min(eps_schedule)\n",
        "    for _ in range(deep_rounds):\n",
        "        current = deep_obfuscate(current)\n",
        "        results[ε_min].append(current)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "bI3DmrqiP1da"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import json5\n",
        "\n",
        "# questions_root = \"/kaggle/input/dataset-ldp/Data_LDP/Attack Question\"\n",
        "questions_root = '/content/Data/Answer Questions '\n",
        "results        = []                                      # collect everything here\n",
        "\n",
        "output_file = '/content/processed_results.json'\n",
        "with open(output_file, 'w') as json_out:\n",
        "    json.dump([], json_out)  # Initialize the file with an empty list\n",
        "\n",
        "    for folder in sorted(os.listdir(questions_root)):\n",
        "        folder_path = os.path.join(questions_root, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        for i in range(1, 11):  # 1 … 10\n",
        "            json_path = os.path.join(folder_path, f\"{i}.json\")\n",
        "            if not os.path.exists(json_path):\n",
        "                continue\n",
        "\n",
        "            with open(json_path, \"r\") as f:\n",
        "                qlist = json5.load(f)\n",
        "\n",
        "            for item in qlist:\n",
        "                # Call the privacy_gate_pipeline function\n",
        "                label = item['label']\n",
        "                text_label = item['answer']\n",
        "                # data = {'resp', text_label}\n",
        "                # json_data = json.dumps(data)\n",
        "                text_label = '[resp]:'++ text_label\n",
        "                print(f\"-----------text_label:{text_label} - label:{label}-----------\")\n",
        "                results = privacy_gate_pipeline(text=text_label, label=label.upper() )\n",
        "\n",
        "                # Prepare the result to be written to the file\n",
        "                processed_item = {'original_item': item, 'pipeline_results': results}\n",
        "\n",
        "                # Open the file again in append mode and write the results immediately\n",
        "                # with open(output_file, 'r+') as json_out:\n",
        "                #     existing_data = json.load(json_out)  # Load the existing data\n",
        "                #     existing_data.append(processed_item)  # Append the new result\n",
        "                #     json_out.seek(0)  # Move cursor to the beginning\n",
        "                #     json.dump(existing_data, json_out, indent=4)  # Write the updated data back\n",
        "\n",
        "                # (optional) show progress in the Kaggle console\n",
        "                print(f\"✓ results : {results}\")\n",
        "\n",
        "print(f\"✓ Processed results saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "ee2sqU6yP1SO",
        "outputId": "939d1e46-2731-49ca-abc7-3fcd62b87fed"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------text_label:respRegularly tracking these metrics helps to identify areas for improvement and ensures that the practices of customer service are in alignment with the goals of the business. - labelConfidential-----------\n",
            "✓ results : {1.0: ['Regularly monitoring these metrics supports identifying improvement opportunities and ensures customer service aligns with business objectives.'], 0.7: ['Regularly reviewing these indicators helps recognize areas for enhancement and maintains alignment with organizational goals.'], 0.5: ['Consistently assessing these factors supports improvement and ensures alignment with objectives.'], 0.3: ['Evaluating aspects helps progress and maintains focus.'], 0.1: ['Assessing supports improvement.', 'Details adjusted, focus shifted, meaning simplified.', 'Things happen, choices made, outcomes follow.', 'Details were altered; events unfolded in unclear ways. Things happened, but specifics faded into obscurity.', 'Details unclear, broad ideas suggested, things loosely mentioned.']}\n",
            "-----------text_label:respExceptional customer service can differentiate a business from its competitors and can also drive customer loyalty and retention. - labelSensitive-----------\n",
            "✓ results : {1.0: ['Exceptional customer service can set a business apart and encourage customer loyalty.'], 0.7: ['Providing strong support to customers can help distinguish a company and foster repeat patronage.'], 0.5: ['Offering assistance to individuals can enhance relationships and encourage continued interaction.'], 0.3: ['Providing help can improve connections and promote engagement.'], 0.1: ['Support fosters interaction.', 'Details removed, general points stay. Words shifted, meaning broad.', 'Things happened, involving certain elements, leading to outcomes that were unclear.', 'Details were adjusted, things altered, intent preserved.', 'Things happened, involving some aspects, leading to unclear outcomes.']}\n",
            "-----------text_label:respReliability helps to build trust, and effective communication ensures that customers are always informed and engaged. - labelSensitive-----------\n",
            "✓ results : {1.0: ['respReliability contributes to trust-building, while clear communication keeps customers updated and involved.'], 0.7: ['Reliable responses help foster trust, and effective communication ensures customers stay informed and engaged.'], 0.5: ['Consistent interactions support confidence, and clear exchanges maintain awareness and involvement.'], 0.3: ['Interactions help and exchanges assist.'], 0.1: ['Support occurs.', 'Details simplified, context adjusted, purpose remains.', 'Details minimized, phrasing altered, meaning stays unclear.', 'Content adjusted for broader meaning and less detail.', 'Things shifted, actions happened, and outcomes followed. Details unclear, roles ambiguous, purposes broad.']}\n",
            "-----------text_label:respA focus on customer satisfaction leads to loyalty in the long-term and is often more cost-effective than the acquisition of new customers. - labelSensitive-----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-44-4282686781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mtext_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'resp'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtext_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"-----------text_label:{text_label} - label{label}-----------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprivacy_gate_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# Prepare the result to be written to the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24-2214242544.py\u001b[0m in \u001b[0;36mprivacy_gate_pipeline\u001b[0;34m(text, label, eps_schedule, deep_rounds)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mε\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meps_schedule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitise_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mε\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24-2214242544.py\u001b[0m in \u001b[0;36msanitise_once\u001b[0;34m(chunk, label, ε)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mε\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\\\\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     )\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLM_PRECISE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# ─────────── 3-B.  deep-obfuscate step (Phase B) ───────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24-2214242544.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(llm, prompt)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     resp = llm(\n\u001b[0m\u001b[1;32m     61\u001b[0m         [SystemMessage(content=\"You are a privacy-focused assistant.\"),\n\u001b[1;32m     62\u001b[0m          HumanMessage(content=prompt)],\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \"\"\"\n\u001b[0;32m-> 1207\u001b[0;31m         generation = self.generate(\n\u001b[0m\u001b[1;32m   1208\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m         ).generations[0][0]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                 results.append(\n\u001b[0;32m--> 782\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    783\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1029\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1086\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1088\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         )\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    980\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6bmhbfCP1Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvSEFM3-P1Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UoEjco5UP1GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aW0d5u_SP1C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFid1blcP0_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zcdyr13OP09C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-NgmaZaP02i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BwAMvmtRP0zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n77aCUOSP0w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YMBalxPuP0uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKYIbuLDP0rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3jgS1PFP0pK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}