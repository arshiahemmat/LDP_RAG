{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('gemini_api')\n",
        "API_KEY = userdata.get('avalai_api')"
      ],
      "metadata": {
        "id": "XGu05kBUAIcu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai python-docx langchain-community langchain-openai json5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxmVp5vTAQld",
        "outputId": "223f2ef4-44e8-4da0-de87-a34594190979"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q json5"
      ],
      "metadata": {
        "id": "4cqlIccjxwRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"14ub55iNBBbZyedB7C1U6y-tC1S_1RRcf\"\n",
        "!unzip Data.zip"
      ],
      "metadata": {
        "id": "F087Hq20SMll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken openai"
      ],
      "metadata": {
        "id": "AjVMj2B473hE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "_vnglclPJCcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q json5"
      ],
      "metadata": {
        "id": "ckSeq0vxt3Rm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####َAdaptive Chunking"
      ],
      "metadata": {
        "id": "72KyYZhBpsoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from docx import Document\n",
        "import re\n",
        "import json\n",
        "import tiktoken\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "CLASSIFICATION_MODEL_NAME = 'gpt-4o-mini'\n",
        "DEFAULT_TAG = 'Public'\n",
        "\n",
        "class SemanticDocumentProcessor:\n",
        "\n",
        "    def __init__(self, api_key, base_url=\"https://api.avalai.ir/v1\"):\n",
        "        \"\"\"Initializes the processor with the OpenAI API and semantic model.\"\"\"\n",
        "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
        "        # Load sentence transformer for semantic similarity\n",
        "        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def _extract_text_from_docx(self, docx_path: str) -> str:\n",
        "        \"\"\"Extracts all text from a DOCX file.\"\"\"\n",
        "        try:\n",
        "            document = Document(docx_path)\n",
        "            return \"\\n\".join(p.text for p in document.paragraphs)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from DOCX: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def _count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Counts the number of tokens in a given text.\"\"\"\n",
        "        try:\n",
        "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "            tokens = encoding.encode(text)\n",
        "            return len(tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"Error counting tokens: {e}. Using fallback estimate.\")\n",
        "            return len(text) // 4\n",
        "\n",
        "    def _split_into_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Splits text into sentences, ensuring punctuation.\"\"\"\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        processed = []\n",
        "        for sentence in sentences:\n",
        "            stripped_sentence = sentence.strip()\n",
        "            if stripped_sentence:\n",
        "                if not re.search(r'[.!?]$', stripped_sentence):\n",
        "                    stripped_sentence += '.'\n",
        "                processed.append(stripped_sentence)\n",
        "        return processed\n",
        "\n",
        "    def _get_semantic_similarity(self, sentences: List[str]) -> np.ndarray:\n",
        "        \"\"\"Compute semantic similarity matrix for sentences.\"\"\"\n",
        "        try:\n",
        "            embeddings = self.semantic_model.encode(sentences)\n",
        "            similarity_matrix = cosine_similarity(embeddings)\n",
        "            return similarity_matrix\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing semantic similarity: {e}\")\n",
        "            return np.eye(len(sentences))  # Return identity matrix as fallback\n",
        "\n",
        "    def _find_semantic_boundaries(self, sentences: List[str], similarity_threshold: float = 0.5) -> List[int]:\n",
        "        \"\"\"Find semantic boundaries between sentences based on similarity.\"\"\"\n",
        "        if len(sentences) <= 1:\n",
        "            return []\n",
        "\n",
        "        similarity_matrix = self._get_semantic_similarity(sentences)\n",
        "        boundaries = []\n",
        "\n",
        "        for i in range(len(sentences) - 1):\n",
        "            # Check similarity between consecutive sentences\n",
        "            similarity = similarity_matrix[i][i + 1]\n",
        "            if similarity < similarity_threshold:\n",
        "                boundaries.append(i + 1)  # Boundary after sentence i\n",
        "\n",
        "        return boundaries\n",
        "\n",
        "    def _build_semantic_chunks(self, sentences: List[str], max_tokens: int,\n",
        "                             similarity_threshold: float = 0.5) -> List[Dict]:\n",
        "        \"\"\"Build chunks based on semantic boundaries while respecting token limits.\"\"\"\n",
        "        if not sentences:\n",
        "            return []\n",
        "\n",
        "        # Find potential semantic boundaries\n",
        "        semantic_boundaries = set(self._find_semantic_boundaries(sentences, similarity_threshold))\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk_sentences = []\n",
        "        current_token_count = 0\n",
        "        current_sentence_ids = []\n",
        "        current_labels = []\n",
        "\n",
        "        for idx, sentence in enumerate(sentences):\n",
        "            sentence_token_count = self._count_tokens(sentence)\n",
        "            sentence_label = self._classify_sentence(sentence)\n",
        "            print(f\"Processing sentence {idx + 1}: {sentence_label}\")\n",
        "\n",
        "            # Check if we need to create a new chunk\n",
        "            should_break = False\n",
        "\n",
        "            if current_chunk_sentences:\n",
        "                # Check if adding this sentence would exceed token limit\n",
        "                if current_token_count + sentence_token_count > max_tokens:\n",
        "                    # If we're at a semantic boundary, break here\n",
        "                    if idx in semantic_boundaries:\n",
        "                        should_break = True\n",
        "                    # If we're way over the limit, force a break\n",
        "                    elif current_token_count + sentence_token_count > max_tokens * 1.2:\n",
        "                        should_break = True\n",
        "                    # Otherwise, try to find the nearest semantic boundary\n",
        "                    else:\n",
        "                        # Look ahead for a nearby semantic boundary\n",
        "                        nearby_boundary = None\n",
        "                        for boundary in semantic_boundaries:\n",
        "                            if idx <= boundary <= min(idx + 3, len(sentences)):\n",
        "                                nearby_boundary = boundary\n",
        "                                break\n",
        "\n",
        "                        if nearby_boundary and nearby_boundary == idx:\n",
        "                            should_break = True\n",
        "\n",
        "            if should_break and current_chunk_sentences:\n",
        "                # Create chunk with current sentences\n",
        "                sentence_labels = {}\n",
        "                for i, (sent_id, label) in enumerate(zip(current_sentence_ids, current_labels)):\n",
        "                    sentence_labels[f\"sentence {i+1}\"] = label.upper()\n",
        "\n",
        "                chunks.append({\n",
        "                    'chunk_id': len(chunks) + 1,\n",
        "                    'tag': self._determine_highest_tag(current_labels),\n",
        "                    'content': ' '.join(current_chunk_sentences).strip(),\n",
        "                    'sentence_labels': sentence_labels,\n",
        "                    'semantic_coherence_score': self._calculate_chunk_coherence(current_chunk_sentences)\n",
        "                })\n",
        "\n",
        "                # Reset for new chunk\n",
        "                current_chunk_sentences = []\n",
        "                current_sentence_ids = []\n",
        "                current_labels = []\n",
        "                current_token_count = 0\n",
        "\n",
        "            # Add current sentence to chunk\n",
        "            current_chunk_sentences.append(sentence)\n",
        "            current_sentence_ids.append(idx + 1)\n",
        "            current_labels.append(sentence_label)\n",
        "            current_token_count += sentence_token_count\n",
        "\n",
        "        # Handle the last chunk\n",
        "        if current_chunk_sentences:\n",
        "            sentence_labels = {}\n",
        "            for i, (sent_id, label) in enumerate(zip(current_sentence_ids, current_labels)):\n",
        "                sentence_labels[f\"sentence {i+1}\"] = label.upper()\n",
        "\n",
        "            chunks.append({\n",
        "                'chunk_id': len(chunks) + 1,\n",
        "                'tag': self._determine_highest_tag(current_labels),\n",
        "                'content': ' '.join(current_chunk_sentences).strip(),\n",
        "                'sentence_labels': sentence_labels,\n",
        "                'semantic_coherence_score': self._calculate_chunk_coherence(current_chunk_sentences)\n",
        "            })\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _calculate_chunk_coherence(self, sentences: List[str]) -> float:\n",
        "        \"\"\"Calculate the semantic coherence score for a chunk.\"\"\"\n",
        "        if len(sentences) <= 1:\n",
        "            return 1.0\n",
        "\n",
        "        try:\n",
        "            similarity_matrix = self._get_semantic_similarity(sentences)\n",
        "            # Calculate average pairwise similarity\n",
        "            mask = np.triu(np.ones_like(similarity_matrix), k=1).astype(bool)\n",
        "            avg_similarity = similarity_matrix[mask].mean()\n",
        "            return float(avg_similarity)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating coherence: {e}\")\n",
        "            return 0.5\n",
        "\n",
        "    def _determine_highest_tag(self, labels: List[str]) -> str:\n",
        "        \"\"\"Determines the highest security level tag from a list of labels.\"\"\"\n",
        "        if 'Confidential' in labels:\n",
        "            return 'CONFIDENTIAL'\n",
        "        elif 'Sensitive' in labels:\n",
        "            return 'SENSITIVE'\n",
        "        else:\n",
        "            return 'PUBLIC'\n",
        "\n",
        "    def _classify_sentence(self, sentence: str) -> str:\n",
        "        \"\"\"Classifies a sentence as Public, Sensitive, or Confidential.\"\"\"\n",
        "        prompt = \"\"\"Classify the following text into one of three security levels:\n",
        "\n",
        "        CLASSIFICATION CRITERIA:\n",
        "        - Public: General information, publicly available knowledge, educational content, non-specific data\n",
        "        - Sensitive: Internal information, specific procedures, mild personal details, internal policies\n",
        "        - Confidential: Personal identifiable information (PII), financial data, medical records, proprietary information, specific patient cases with identifying details\n",
        "\n",
        "        Respond with only one word: 'Public', 'Sensitive', or 'Confidential'.\n",
        "\n",
        "        Text: \\\"\\\"\\\"{}\\\"\\\"\\\"\"\"\".format(sentence)\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=CLASSIFICATION_MODEL_NAME,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=10,\n",
        "                temperature=0.0\n",
        "            )\n",
        "            tag = response.choices[0].message.content.strip()\n",
        "            if tag in ['Public', 'Sensitive', 'Confidential']:\n",
        "                return tag\n",
        "            else:\n",
        "                return DEFAULT_TAG\n",
        "        except Exception as e:\n",
        "            print(f\"Error classifying sentence: {e}. Defaulting to '{DEFAULT_TAG}'.\")\n",
        "            return DEFAULT_TAG\n",
        "\n",
        "    def _save_to_json(self, chunks_data: List[Dict], output_filename: str):\n",
        "        \"\"\"Saves the chunked data to a JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(output_filename, 'w', encoding='utf-8') as file:\n",
        "                json.dump(chunks_data, file, indent=4, ensure_ascii=False)\n",
        "            print(f\"Chunked data saved to {output_filename}\")\n",
        "        except IOError as e:\n",
        "            print(f\"Error saving to JSON file: {e}\")\n",
        "\n",
        "    def process(self, docx_path: str, output_filename: str, max_tokens: int = 500,\n",
        "                similarity_threshold: float = 0.5):\n",
        "        \"\"\"The main method to process a DOCX file with semantic chunking.\"\"\"\n",
        "        text = self._extract_text_from_docx(docx_path)\n",
        "        if not text:\n",
        "            print(\"No text to process. Exiting.\")\n",
        "            return\n",
        "\n",
        "        sentences = self._split_into_sentences(text)\n",
        "        chunks = self._build_semantic_chunks(sentences, max_tokens, similarity_threshold)\n",
        "\n",
        "        self._save_to_json(chunks, output_filename)\n",
        "\n",
        "        # Print summary statistics\n",
        "        total_sentences = sum(len(chunk['sentence_labels']) for chunk in chunks)\n",
        "        public_count = sum(1 for chunk in chunks for label in chunk['sentence_labels'].values() if label == 'PUBLIC')\n",
        "        sensitive_count = sum(1 for chunk in chunks for label in chunk['sentence_labels'].values() if label == 'SENSITIVE')\n",
        "        confidential_count = sum(1 for chunk in chunks for label in chunk['sentence_labels'].values() if label == 'CONFIDENTIAL')\n",
        "\n",
        "        avg_coherence = np.mean([chunk['semantic_coherence_score'] for chunk in chunks])\n",
        "\n",
        "        print(f\"\\n--- Classification Summary ---\")\n",
        "        print(f\"Total sentences: {total_sentences}\")\n",
        "        print(f\"Total chunks: {len(chunks)}\")\n",
        "        print(f\"Average semantic coherence: {avg_coherence:.3f}\")\n",
        "        print(f\"PUBLIC: {public_count}\")\n",
        "        print(f\"SENSITIVE: {sensitive_count}\")\n",
        "        print(f\"CONFIDENTIAL: {confidential_count}\")\n",
        "\n",
        "\n",
        "# Alternative approach using topic modeling\n",
        "class TopicBasedChunker(SemanticDocumentProcessor):\n",
        "    \"\"\"Enhanced version using topic modeling for better semantic boundaries.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key, base_url=\"https://api.avalai.ir/v1\"):\n",
        "        super().__init__(api_key, base_url)\n",
        "\n",
        "    def _identify_topics(self, sentences: List[str]) -> List[str]:\n",
        "        \"\"\"Identify topics for sentences using LLM.\"\"\"\n",
        "        if not sentences:\n",
        "            return []\n",
        "\n",
        "        # Process in batches to avoid token limits\n",
        "        topics = []\n",
        "        batch_size = 10\n",
        "\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i+batch_size]\n",
        "            batch_text = \"\\n\".join([f\"{idx+1}. {sent}\" for idx, sent in enumerate(batch)])\n",
        "\n",
        "            prompt = f\"\"\"Analyze the following sentences and assign a brief topic/theme to each (1-3 words max).\n",
        "\n",
        "            Sentences:\n",
        "            {batch_text}\n",
        "\n",
        "            Respond in JSON format:\n",
        "            {{\"topics\": [\"topic1\", \"topic2\", ...]}}\"\"\"\n",
        "\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=CLASSIFICATION_MODEL_NAME,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=200,\n",
        "                    temperature=0.1\n",
        "                )\n",
        "\n",
        "                result = json.loads(response.choices[0].message.content.strip())\n",
        "                topics.extend(result.get(\"topics\", [\"general\"] * len(batch)))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error identifying topics: {e}\")\n",
        "                topics.extend([\"general\"] * len(batch))\n",
        "\n",
        "        return topics\n",
        "\n",
        "    def _find_topic_boundaries(self, topics: List[str]) -> List[int]:\n",
        "        \"\"\"Find boundaries where topics change.\"\"\"\n",
        "        boundaries = []\n",
        "        for i in range(len(topics) - 1):\n",
        "            if topics[i] != topics[i + 1]:\n",
        "                boundaries.append(i + 1)\n",
        "        return boundaries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k628YMvytWBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example:\n",
        "if __name__ == \"__main__\":\n",
        "    processor = SemanticDocumentProcessor(api_key=API_KEY)\n",
        "    processor.process(\n",
        "        docx_path=\"/content/Data/Docs/Customer Service/1.docx\",\n",
        "        output_filename=\"semantic_chunks_new.json\",\n",
        "        max_tokens=256,\n",
        "        similarity_threshold=0.6  # Adjust for more/less strict semantic boundaries\n",
        "    )"
      ],
      "metadata": {
        "id": "vxuxlb77tltc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SCBsQvSLtliP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Privacy Gateway"
      ],
      "metadata": {
        "id": "l9_IGoIiCeSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Privacy Gateway"
      ],
      "metadata": {
        "id": "LTMlXKmZpwFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "# ─────────── 0.  MODEL HANDLES ──────────────────────────────\n",
        "def make_llm(temp: float) -> ChatOpenAI:\n",
        "    return ChatOpenAI(\n",
        "        base_url = \"https://api.avalai.ir/v1\",\n",
        "        model=\"gpt-4o\",\n",
        "        api_key=API_KEY,\n",
        "        temperature=temp,\n",
        "    )\n",
        "\n",
        "LLM_PRECISE = make_llm(0.0)   # Phase A\n",
        "LLM_DEEP    = make_llm(0.7)   # Phase B\n",
        "\n",
        "MAX_TOKENS = 512\n",
        "\n",
        "# ─────────── 1-A.  PRECISION PROMPT  (Phase A) ─────────────\n",
        "PRECISION_TMPL = \"\"\"\n",
        "You are the Privacy Gate.\n",
        "\n",
        "Rewrite the text so that its vagueness matches ε = {epsilon}.\n",
        "\n",
        "Guidelines\n",
        "• ε = 1.0  → keep almost all specifics; redact only obvious secrets\n",
        "• ε = 0.7  → generalise a little\n",
        "• ε = 0.5  → remove/blur most specifics\n",
        "• ε = 0.3  → keep only broad actions\n",
        "• ε = 0.1  → bare outline\n",
        "\n",
        "Do NOT remove the core meaning.\n",
        "\n",
        "Label: {label}\n",
        "Original text:\n",
        "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
        "\n",
        "Return JSON only:\n",
        "{{\"rewritten\": \"<sanitised>\"}}\"\"\"\n",
        "\n",
        "# ─────────── 1-B.  DEEP-OBFUSCATE PROMPT (Phase B) ─────────\n",
        "DEEP_TMPL = \"\"\"\n",
        "You are the Privacy Gate – deep-obfuscation mode.\n",
        "\n",
        "Take the input text and rewrite it so it is **even vaguer** than before:\n",
        "• shorten sentences,\n",
        "• replace any remaining specifics with generic terms,\n",
        "• rephrase with **different wording** than the previous version,\n",
        "• keep overall intent.\n",
        "\n",
        "Return JSON only:\n",
        "{{\"rewritten\": \"<even more vague text>\"}}\"\"\"\n",
        "\n",
        "# ─────────── 2.  LOW-LEVEL CALL HELPERS ─────────────────────\n",
        "_JSON_RE = re.compile(r\"\\{.*\\}\", re.DOTALL)\n",
        "\n",
        "def _call(llm: ChatOpenAI, prompt: str) -> str:\n",
        "    resp = llm(\n",
        "        [SystemMessage(content=\"You are a privacy-focused assistant.\"),\n",
        "         HumanMessage(content=prompt)],\n",
        "        max_tokens=MAX_TOKENS,\n",
        "    ).content.strip()\n",
        "    print(f\"Raw LLM response: {resp}\") # Added print for debugging\n",
        "    m = _JSON_RE.search(resp)\n",
        "    if not m:\n",
        "        raise ValueError(f\"Missing JSON:\\n{resp}\")\n",
        "    json_string = m.group(0) # Extracted JSON string\n",
        "    print(f\"Extracted JSON string: {json_string}\") # Added print for debugging\n",
        "    obj = json.loads(json_string) # Parse the extracted string\n",
        "    if \"rewritten\" not in obj:\n",
        "        raise ValueError(f\"No 'rewritten' key:\\n{obj}\")\n",
        "    return obj[\"rewritten\"]\n",
        "\n",
        "# ─────────── 3-A.  single-pass sanitiser (Phase A) ─────────\n",
        "def sanitise_once(chunk: str, label: Literal[\"PUBLIC\", \"SENSITIVE\", \"CONFIDENTIAL\"], ε: float) -> str:\n",
        "    prompt = PRECISION_TMPL.format(\n",
        "        label=label, epsilon=ε, chunk=chunk.replace('\"', '\\\\\"')\n",
        "    )\n",
        "    return _call(LLM_PRECISE, prompt)\n",
        "\n",
        "# ─────────── 3-B.  deep-obfuscate step (Phase B) ───────────\n",
        "def deep_obfuscate(prev: str) -> str:\n",
        "    prompt = DEEP_TMPL.format(chunk=prev.replace('\"', '\\\\\"'))\n",
        "    return _call(LLM_DEEP, prompt)\n",
        "\n",
        "# ─────────── 4.  public pipeline ───────────────────────────\n",
        "def privacy_gate_pipeline(\n",
        "    text: str,\n",
        "    label: Literal[\"PUBLIC\", \"SENSITIVE\", \"CONFIDENTIAL\"],\n",
        "    eps_schedule: List[float] = (1.0, 0.7, 0.5, 0.3, 0.1),\n",
        "    deep_rounds: int = 4,\n",
        ") -> Dict[float, List[str]]:\n",
        "    \"\"\"\n",
        "    Phase A: run once for every ε in eps_schedule\n",
        "    Phase B: take ε=min(eps_schedule) output, run 'deep_rounds' extra passes\n",
        "    Returns {ε: [v1, v2, …]}  (ε>min → one element; ε_min → deep_rounds+1 elems)\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    current = text\n",
        "    for ε in eps_schedule:\n",
        "        current = sanitise_once(current, label, ε)\n",
        "        results[ε] = [current]\n",
        "\n",
        "    ε_min = min(eps_schedule)\n",
        "    for _ in range(deep_rounds):\n",
        "        current = deep_obfuscate(current)\n",
        "        results[ε_min].append(current)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "bI3DmrqiP1da"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import json5\n",
        "\n",
        "output_file = \"/content/output_file.json\"\n",
        "question_root = \"\"\n",
        "\n",
        "def split_into_sentences( text: str) -> list[str]:\n",
        "    \"\"\"Splits text into sentences, ensuring punctuation.\"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    processed = []\n",
        "    for sentence in sentences:\n",
        "        stripped_sentence = sentence.strip()\n",
        "        if stripped_sentence:\n",
        "            if not re.search(r'[.!?]$', stripped_sentence):\n",
        "                stripped_sentence += '.'\n",
        "            processed.append(stripped_sentence)\n",
        "    return processed\n",
        "\n",
        "with open(output_file, 'w') as json_out:\n",
        "    json.dump([], json_out)  # Initialize the file with an empty list\n",
        "\n",
        "    # for folder in sorted(os.listdir(questions_root)):\n",
        "        # folder_path = os.path.join(questions_root, folder)\n",
        "        # if not os.path.isdir(folder_path):\n",
        "        #     continue\n",
        "\n",
        "        # for i in range(1, 11):  # 1 … 10\n",
        "        #     json_path = os.path.join(folder_path, f\"{i}.json\")\n",
        "        #     if not os.path.exists(json_path):\n",
        "        #         continue\n",
        "\n",
        "    json_path = \"/content/semantic_chunks_new.json\"\n",
        "    with open(json_path, \"r\") as f:\n",
        "        qlist = json5.load(f)\n",
        "\n",
        "    for item in qlist:\n",
        "        sentences = split_into_sentences(item[\"content\"])\n",
        "        sentences_labels = item['sentence_labels']\n",
        "\n",
        "        for idx in range(len(sentences)):\n",
        "          # Construct the correct key for accessing the dictionary\n",
        "          label_key = f\"sentence {idx+1}\"\n",
        "\n",
        "          sentence_result = privacy_gate_pipeline(\n",
        "              text = sentences[idx],\n",
        "              label = sentences_labels[label_key] # Use the constructed key here\n",
        "          )\n",
        "          with open(output_file, 'w') as json_out:\n",
        "              json.dump([], json_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QFc5oDdOqErh",
        "outputId": "70283627-8e90-4a10-e0e2-4952f1a32717"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw LLM response: ```json\n",
            "{\n",
            "  \"rewritten\": \"Navigating Chronic Kidney Disease: Prevention, Management, and Care\\nTable of Contents\\nIntroduction to Chronic Kidney Disease (CKD)\\n\\nAnatomy and Function of the Kidneys\\n\\nCauses and Risk Factors\\n\\nStages of CKD\\n\\nSigns and Symptoms\\n\\nScreening and Diagnostic Procedures\\n\\nNutritional Management\\n\\nMedical Management and Pharmacologic Therapies\\n\\nDialysis Overview\\n\\nKidney Transplant Considerations\\n\\nPatient Education and Lifestyle Adjustments\\n\\nCKD and Comorbidities\\n\\nCase Study: [Redacted].\"\n",
            "}\n",
            "```\n",
            "Extracted JSON string: {\n",
            "  \"rewritten\": \"Navigating Chronic Kidney Disease: Prevention, Management, and Care\\nTable of Contents\\nIntroduction to Chronic Kidney Disease (CKD)\\n\\nAnatomy and Function of the Kidneys\\n\\nCauses and Risk Factors\\n\\nStages of CKD\\n\\nSigns and Symptoms\\n\\nScreening and Diagnostic Procedures\\n\\nNutritional Management\\n\\nMedical Management and Pharmacologic Therapies\\n\\nDialysis Overview\\n\\nKidney Transplant Considerations\\n\\nPatient Education and Lifestyle Adjustments\\n\\nCKD and Comorbidities\\n\\nCase Study: [Redacted].\"\n",
            "}\n",
            "Raw LLM response: ```json\n",
            "{\"rewritten\": \"Navigating Chronic Kidney Disease: Prevention, Management, and Care\\nTable of Contents\\nIntroduction to Kidney Health\\nBasic Kidney Functions\\nPotential Causes and Risks\\nProgression of Kidney Conditions\\nCommon Indicators\\nEvaluation Methods\\nDietary Guidance\\nTreatment Options\\nOverview of Dialysis\\nTransplant Basics\\nHealth Education and Daily Habits\\nKidney Health and Related Conditions\\nCase Study: [Generalised Example].\"}\n",
            "```\n",
            "Extracted JSON string: {\"rewritten\": \"Navigating Chronic Kidney Disease: Prevention, Management, and Care\\nTable of Contents\\nIntroduction to Kidney Health\\nBasic Kidney Functions\\nPotential Causes and Risks\\nProgression of Kidney Conditions\\nCommon Indicators\\nEvaluation Methods\\nDietary Guidance\\nTreatment Options\\nOverview of Dialysis\\nTransplant Basics\\nHealth Education and Daily Habits\\nKidney Health and Related Conditions\\nCase Study: [Generalised Example].\"}\n",
            "Raw LLM response: ```json\n",
            "{\"rewritten\": \"Navigating Kidney Health: Prevention, Management, and Care\\nTable of Contents\\nIntroduction to Organ Health\\nBasic Functions\\nPotential Factors and Risks\\nProgression of Conditions\\nGeneral Indicators\\nEvaluation Approaches\\nDietary Tips\\nCare Options\\nOverview of Procedures\\nTransplant Essentials\\nHealth Practices and Habits\\nOrgan Health and Related Issues\\nCase Study: [Example].\"}\n",
            "```\n",
            "Extracted JSON string: {\"rewritten\": \"Navigating Kidney Health: Prevention, Management, and Care\\nTable of Contents\\nIntroduction to Organ Health\\nBasic Functions\\nPotential Factors and Risks\\nProgression of Conditions\\nGeneral Indicators\\nEvaluation Approaches\\nDietary Tips\\nCare Options\\nOverview of Procedures\\nTransplant Essentials\\nHealth Practices and Habits\\nOrgan Health and Related Issues\\nCase Study: [Example].\"}\n",
            "Raw LLM response: ```json\n",
            "{\"rewritten\": \"Maintaining Health: Broad Actions and Considerations\"}\n",
            "```\n",
            "Extracted JSON string: {\"rewritten\": \"Maintaining Health: Broad Actions and Considerations\"}\n",
            "Raw LLM response: ```json\n",
            "{\"rewritten\": \"Focus on well-being through general practices.\"}\n",
            "```\n",
            "Extracted JSON string: {\"rewritten\": \"Focus on well-being through general practices.\"}\n",
            "Raw LLM response: {\"rewritten\": \"Details were altered, ideas shifted, focus blurred for broader meaning.\"}\n",
            "Extracted JSON string: {\"rewritten\": \"Details were altered, ideas shifted, focus blurred for broader meaning.\"}\n",
            "Raw LLM response: {\"rewritten\": \"Things happened, involving some stuff, leading to certain outcomes.\"}\n",
            "Extracted JSON string: {\"rewritten\": \"Things happened, involving some stuff, leading to certain outcomes.\"}\n",
            "Raw LLM response: {\"rewritten\": \"Simplify details, shift wording, keep meaning unclear.\"}\n",
            "Extracted JSON string: {\"rewritten\": \"Simplify details, shift wording, keep meaning unclear.\"}\n",
            "Raw LLM response: {\"rewritten\": \"Things happened, involving some aspects, leading to outcomes.\"}\n",
            "Extracted JSON string: {\"rewritten\": \"Things happened, involving some aspects, leading to outcomes.\"}\n",
            "Raw LLM response: ```json\n",
            "{\"rewritten\": \"Case Study: Fatima L.\"}\n",
            "```\n",
            "Extracted JSON string: {\"rewritten\": \"Case Study: Fatima L.\"}\n",
            "Raw LLM response: ```json\n",
            "{\"rewritten\": \"Case Study: Individual F.\"}\n",
            "```\n",
            "Extracted JSON string: {\"rewritten\": \"Case Study: Individual F.\"}\n",
            "Raw LLM response: ```json\n",
            "{\"rewritten\": \"Case Study: A person was involved in a situation.\"}\n",
            "```\n",
            "Extracted JSON string: {\"rewritten\": \"Case Study: A person was involved in a situation.\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-17-3419019890.py\", line 45, in <cell line: 0>\n",
            "    sentence_result = privacy_gate_pipeline(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-16-2353152988.py\", line 103, in privacy_gate_pipeline\n",
            "    current = sanitise_once(current, label, ε)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-16-2353152988.py\", line 81, in sanitise_once\n",
            "    return _call(LLM_PRECISE, prompt)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-16-2353152988.py\", line 60, in _call\n",
            "    resp = llm(\n",
            "           ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\", line 189, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 1207, in __call__\n",
            "    generation = self.generate(\n",
            "                 ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 782, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 1028, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\", line 1131, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\", line 1087, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1256, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 979, in request\n",
            "    response = self._client.send(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 914, in send\n",
            "    response = self._send_handling_auth(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
            "    response = connection.handle_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\", line 103, in handle_request\n",
            "    return self._connection.handle_request(request)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\", line 136, in handle_request\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\", line 106, in handle_request\n",
            "    ) = self._receive_response_headers(**kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\", line 177, in _receive_response_headers\n",
            "    event = self._receive_event(timeout=timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\", line 217, in _receive_event\n",
            "    data = self._network_stream.read(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\", line 128, in read\n",
            "    return self._sock.recv(max_bytes)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1295, in recv\n",
            "    return self.read(buflen)\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1168, in read\n",
            "    return self._sslobj.read(len)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1688, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
            "    file = getsourcefile(object) or getfile(object)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 990, in getmodule\n",
            "    if f == _filesbymodname.get(modname, None):\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-17-3419019890.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m           sentence_result = privacy_gate_pipeline(\n\u001b[0m\u001b[1;32m     46\u001b[0m               \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-16-2353152988.py\u001b[0m in \u001b[0;36mprivacy_gate_pipeline\u001b[0;34m(text, label, eps_schedule, deep_rounds)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mε\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meps_schedule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitise_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mε\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-16-2353152988.py\u001b[0m in \u001b[0;36msanitise_once\u001b[0;34m(chunk, label, ε)\u001b[0m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLM_PRECISE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-16-2353152988.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(llm, prompt)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     resp = llm(\n\u001b[0m\u001b[1;32m     61\u001b[0m         [SystemMessage(content=\"You are a privacy-focused assistant.\"),\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \"\"\"\n\u001b[0;32m-> 1207\u001b[0;31m         generation = self.generate(\n\u001b[0m\u001b[1;32m   1208\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m                 results.append(\n\u001b[0;32m--> 782\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    783\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1029\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1088\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         )\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    980\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ulxGvWXvqEox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import json5\n",
        "import re\n",
        "\n",
        "def split_into_sentences( text: str) -> list[str]:\n",
        "    \"\"\"Splits text into sentences, ensuring punctuation.\"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    processed = []\n",
        "    for sentence in sentences:\n",
        "        stripped_sentence = sentence.strip()\n",
        "        if stripped_sentence:\n",
        "            if not re.search(r'[.!?]$', stripped_sentence):\n",
        "                stripped_sentence += '.'\n",
        "            processed.append(stripped_sentence)\n",
        "    return processed\n",
        "\n",
        "chunks_path = '/content/classified_chunks_labeled(5).json'\n",
        "results = []\n",
        "output_file = '/content/processed_results(1).json'\n",
        "with open(output_file, 'w') as json_out:\n",
        "  json.dump([], json_out)  # Initialize the file with an empty list\n",
        "\n",
        "  with open(json_path, \"r\") as file:\n",
        "    qlist = json5.load(file)\n",
        "\n",
        "  for item in qlist:\n",
        "    sentences = item['content']\n",
        "    processed_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cSuqzzE2M3ZG",
        "outputId": "1fae23c1-e48c-4aef-cbb6-02fdabd6aa07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'json_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-87-2002192332.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_out\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize the file with an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mqlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'json_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import json5\n",
        "\n",
        "# questions_root = \"/kaggle/input/dataset-ldp/Data_LDP/Attack Question\"\n",
        "questions_root = '/content/Data/Answer Questions '\n",
        "results        = []                                      # collect everything here\n",
        "\n",
        "output_file = '/content/processed_results.json'\n",
        "with open(output_file, 'w') as json_out:\n",
        "    json.dump([], json_out)  # Initialize the file with an empty list\n",
        "\n",
        "    for folder in sorted(os.listdir(questions_root)):\n",
        "        folder_path = os.path.join(questions_root, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        for i in range(1, 11):  # 1 … 10\n",
        "            json_path = os.path.join(folder_path, f\"{i}.json\")\n",
        "            if not os.path.exists(json_path):\n",
        "                continue\n",
        "\n",
        "            with open(json_path, \"r\") as f:\n",
        "                qlist = json5.load(f)\n",
        "\n",
        "            for item in qlist:\n",
        "                # Call the privacy_gate_pipeline function\n",
        "                label = item['label']\n",
        "                text_label = item['answer']\n",
        "                # data = {'resp', text_label}\n",
        "                # json_data = json.dumps(data)\n",
        "                text_label = '[resp]:'++ text_label\n",
        "                print(f\"-----------text_label:{text_label} - label:{label}-----------\")\n",
        "                results = privacy_gate_pipeline(text=text_label, label=label.upper() )\n",
        "\n",
        "                # Prepare the result to be written to the file\n",
        "                processed_item = {'original_item': item, 'pipeline_results': results}\n",
        "\n",
        "                # Open the file again in append mode and write the results immediately\n",
        "                # with open(output_file, 'r+') as json_out:\n",
        "                #     existing_data = json.load(json_out)  # Load the existing data\n",
        "                #     existing_data.append(processed_item)  # Append the new result\n",
        "                #     json_out.seek(0)  # Move cursor to the beginning\n",
        "                #     json.dump(existing_data, json_out, indent=4)  # Write the updated data back\n",
        "\n",
        "                # (optional) show progress in the Kaggle console\n",
        "                print(f\"✓ results : {results}\")\n",
        "\n",
        "print(f\"✓ Processed results saved to {output_file}\")"
      ],
      "metadata": {
        "id": "ee2sqU6yP1SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6bmhbfCP1Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvSEFM3-P1Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UoEjco5UP1GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aW0d5u_SP1C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFid1blcP0_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zcdyr13OP09C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-NgmaZaP02i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BwAMvmtRP0zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n77aCUOSP0w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YMBalxPuP0uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKYIbuLDP0rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3jgS1PFP0pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "157b4ef9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}