{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da46b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from docx import Document\n",
    "import re\n",
    "import json\n",
    "import tiktoken\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "CLASSIFICATION_MODEL_NAME = 'gpt-4o-mini'\n",
    "DEFAULT_TAG = 'Public'\n",
    "\n",
    "class SemanticDocumentProcessor:\n",
    "\n",
    "    def __init__(self, api_key, base_url=\"https://api.avalai.ir/v1\"):\n",
    "        \"\"\"Initializes the processor with the OpenAI API and semantic model.\"\"\"\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        # Load sentence transformer for semantic similarity\n",
    "        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def _extract_text_from_docx(self, docx_path: str) -> str:\n",
    "        \"\"\"Extracts all text from a DOCX file.\"\"\"\n",
    "        try:\n",
    "            document = Document(docx_path)\n",
    "            return \"\\n\".join(p.text for p in document.paragraphs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from DOCX: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Counts the number of tokens in a given text.\"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            tokens = encoding.encode(text)\n",
    "            return len(tokens)\n",
    "        except Exception as e:\n",
    "            print(f\"Error counting tokens: {e}. Using fallback estimate.\")\n",
    "            return len(text) // 4\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Splits text into sentences, ensuring punctuation.\"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        processed = []\n",
    "        for sentence in sentences:\n",
    "            stripped_sentence = sentence.strip()\n",
    "            if stripped_sentence:\n",
    "                if not re.search(r'[.!?]$', stripped_sentence):\n",
    "                    stripped_sentence += '.'\n",
    "                processed.append(stripped_sentence)\n",
    "        return processed\n",
    "\n",
    "    def _get_semantic_similarity(self, sentences: List[str]) -> np.ndarray:\n",
    "        \"\"\"Compute semantic similarity matrix for sentences.\"\"\"\n",
    "        try:\n",
    "            embeddings = self.semantic_model.encode(sentences)\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            return similarity_matrix\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing semantic similarity: {e}\")\n",
    "            return np.eye(len(sentences))  # Return identity matrix as fallback\n",
    "\n",
    "    def _find_semantic_boundaries(self, sentences: List[str], similarity_threshold: float = 0.5) -> List[int]:\n",
    "        \"\"\"Find semantic boundaries between sentences based on similarity.\"\"\"\n",
    "        if len(sentences) <= 1:\n",
    "            return []\n",
    "        \n",
    "        similarity_matrix = self._get_semantic_similarity(sentences)\n",
    "        boundaries = []\n",
    "        \n",
    "        for i in range(len(sentences) - 1):\n",
    "            # Check similarity between consecutive sentences\n",
    "            similarity = similarity_matrix[i][i + 1]\n",
    "            if similarity < similarity_threshold:\n",
    "                boundaries.append(i + 1)  # Boundary after sentence i\n",
    "        \n",
    "        return boundaries\n",
    "\n",
    "    def _build_semantic_chunks(self, sentences: List[str], max_tokens: int, \n",
    "                             similarity_threshold: float = 0.5) -> List[Dict]:\n",
    "        \"\"\"Build chunks based on semantic boundaries while respecting token limits.\"\"\"\n",
    "        if not sentences:\n",
    "            return []\n",
    "        \n",
    "        # Find potential semantic boundaries\n",
    "        semantic_boundaries = set(self._find_semantic_boundaries(sentences, similarity_threshold))\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk_sentences = []\n",
    "        current_token_count = 0\n",
    "        current_sentence_ids = []\n",
    "        current_labels = []\n",
    "\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            sentence_token_count = self._count_tokens(sentence)\n",
    "            sentence_label = self._classify_sentence(sentence)\n",
    "            print(f\"Processing sentence {idx + 1}: {sentence_label}\")\n",
    "\n",
    "            # Check if we need to create a new chunk\n",
    "            should_break = False\n",
    "            \n",
    "            if current_chunk_sentences:\n",
    "                # Check if adding this sentence would exceed token limit\n",
    "                if current_token_count + sentence_token_count > max_tokens:\n",
    "                    # If we're at a semantic boundary, break here\n",
    "                    if idx in semantic_boundaries:\n",
    "                        should_break = True\n",
    "                    # If we're way over the limit, force a break\n",
    "                    elif current_token_count + sentence_token_count > max_tokens * 1.2:\n",
    "                        should_break = True\n",
    "                    # Otherwise, try to find the nearest semantic boundary\n",
    "                    else:\n",
    "                        # Look ahead for a nearby semantic boundary\n",
    "                        nearby_boundary = None\n",
    "                        for boundary in semantic_boundaries:\n",
    "                            if idx <= boundary <= min(idx + 3, len(sentences)):\n",
    "                                nearby_boundary = boundary\n",
    "                                break\n",
    "                        \n",
    "                        if nearby_boundary and nearby_boundary == idx:\n",
    "                            should_break = True\n",
    "\n",
    "            if should_break and current_chunk_sentences:\n",
    "                # Create chunk with current sentences\n",
    "                sentence_labels = {}\n",
    "                for i, (sent_id, label) in enumerate(zip(current_sentence_ids, current_labels)):\n",
    "                    sentence_labels[f\"sentence {i+1}\"] = label.upper()\n",
    "                \n",
    "                chunks.append({\n",
    "                    'chunk_id': len(chunks) + 1,\n",
    "                    'tag': self._determine_highest_tag(current_labels),\n",
    "                    'content': ' '.join(current_chunk_sentences).strip(),\n",
    "                    'sentence_labels': sentence_labels,\n",
    "                    'semantic_coherence_score': self._calculate_chunk_coherence(current_chunk_sentences)\n",
    "                })\n",
    "                \n",
    "                # Reset for new chunk\n",
    "                current_chunk_sentences = []\n",
    "                current_sentence_ids = []\n",
    "                current_labels = []\n",
    "                current_token_count = 0\n",
    "\n",
    "            # Add current sentence to chunk\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_sentence_ids.append(idx + 1)\n",
    "            current_labels.append(sentence_label)\n",
    "            current_token_count += sentence_token_count\n",
    "\n",
    "        # Handle the last chunk\n",
    "        if current_chunk_sentences:\n",
    "            sentence_labels = {}\n",
    "            for i, (sent_id, label) in enumerate(zip(current_sentence_ids, current_labels)):\n",
    "                sentence_labels[f\"sentence {i+1}\"] = label.upper()\n",
    "            \n",
    "            chunks.append({\n",
    "                'chunk_id': len(chunks) + 1,\n",
    "                'tag': self._determine_highest_tag(current_labels),\n",
    "                'content': ' '.join(current_chunk_sentences).strip(),\n",
    "                'sentence_labels': sentence_labels,\n",
    "                'semantic_coherence_score': self._calculate_chunk_coherence(current_chunk_sentences)\n",
    "            })\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _calculate_chunk_coherence(self, sentences: List[str]) -> float:\n",
    "        \"\"\"Calculate the semantic coherence score for a chunk.\"\"\"\n",
    "        if len(sentences) <= 1:\n",
    "            return 1.0\n",
    "        \n",
    "        try:\n",
    "            similarity_matrix = self._get_semantic_similarity(sentences)\n",
    "            # Calculate average pairwise similarity\n",
    "            mask = np.triu(np.ones_like(similarity_matrix), k=1).astype(bool)\n",
    "            avg_similarity = similarity_matrix[mask].mean()\n",
    "            return float(avg_similarity)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating coherence: {e}\")\n",
    "            return 0.5\n",
    "\n",
    "    def _determine_highest_tag(self, labels: List[str]) -> str:\n",
    "        \"\"\"Determines the highest security level tag from a list of labels.\"\"\"\n",
    "        if 'Confidential' in labels:\n",
    "            return 'CONFIDENTIAL'\n",
    "        elif 'Sensitive' in labels:\n",
    "            return 'SENSITIVE'\n",
    "        else:\n",
    "            return 'PUBLIC'\n",
    "\n",
    "    def _classify_sentence(self, sentence: str) -> str:\n",
    "        \"\"\"Classifies a sentence as Public, Sensitive, or Confidential.\"\"\"\n",
    "        prompt = \"\"\"Classify the following text into one of three security levels:\n",
    "\n",
    "        CLASSIFICATION CRITERIA:\n",
    "        - Public: General information, publicly available knowledge, educational content, non-specific data\n",
    "        - Sensitive: Internal information, specific procedures, mild personal details, internal policies  \n",
    "        - Confidential: Personal identifiable information (PII), financial data, medical records, proprietary information, specific patient cases with identifying details\n",
    "\n",
    "        Respond with only one word: 'Public', 'Sensitive', or 'Confidential'.\n",
    "\n",
    "        Text: \\\"\\\"\\\"{}\\\"\\\"\\\"\"\"\".format(sentence)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CLASSIFICATION_MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=10,\n",
    "                temperature=0.0\n",
    "            )\n",
    "            tag = response.choices[0].message.content.strip()\n",
    "            if tag in ['Public', 'Sensitive', 'Confidential']:\n",
    "                return tag\n",
    "            else:\n",
    "                return DEFAULT_TAG\n",
    "        except Exception as e:\n",
    "            print(f\"Error classifying sentence: {e}. Defaulting to '{DEFAULT_TAG}'.\")\n",
    "            return DEFAULT_TAG\n",
    "\n",
    "    def _save_to_json(self, chunks_data: List[Dict], output_filename: str):\n",
    "        \"\"\"Saves the chunked data to a JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(output_filename, 'w', encoding='utf-8') as file:\n",
    "                json.dump(chunks_data, file, indent=4, ensure_ascii=False)\n",
    "            print(f\"Chunked data saved to {output_filename}\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving to JSON file: {e}\")\n",
    "\n",
    "    def process(self, docx_path: str, output_filename: str, max_tokens: int = 500, \n",
    "                similarity_threshold: float = 0.5):\n",
    "        \"\"\"The main method to process a DOCX file with semantic chunking.\"\"\"\n",
    "        text = self._extract_text_from_docx(docx_path)\n",
    "        if not text:\n",
    "            print(\"No text to process. Exiting.\")\n",
    "            return\n",
    "\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        chunks = self._build_semantic_chunks(sentences, max_tokens, similarity_threshold)\n",
    "\n",
    "        self._save_to_json(chunks, output_filename)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        total_sentences = sum(len(chunk['sentence_labels']) for chunk in chunks)\n",
    "        public_count = sum(1 for chunk in chunks for label in chunk['sentence_labels'].values() if label == 'PUBLIC')\n",
    "        sensitive_count = sum(1 for chunk in chunks for label in chunk['sentence_labels'].values() if label == 'SENSITIVE')\n",
    "        confidential_count = sum(1 for chunk in chunks for label in chunk['sentence_labels'].values() if label == 'CONFIDENTIAL')\n",
    "        \n",
    "        avg_coherence = np.mean([chunk['semantic_coherence_score'] for chunk in chunks])\n",
    "        \n",
    "        print(f\"\\n--- Classification Summary ---\")\n",
    "        print(f\"Total sentences: {total_sentences}\")\n",
    "        print(f\"Total chunks: {len(chunks)}\")\n",
    "        print(f\"Average semantic coherence: {avg_coherence:.3f}\")\n",
    "        print(f\"PUBLIC: {public_count}\")\n",
    "        print(f\"SENSITIVE: {sensitive_count}\")\n",
    "        print(f\"CONFIDENTIAL: {confidential_count}\")\n",
    "\n",
    "\n",
    "# Alternative approach using topic modeling\n",
    "class TopicBasedChunker(SemanticDocumentProcessor):\n",
    "    \"\"\"Enhanced version using topic modeling for better semantic boundaries.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, base_url=\"https://api.avalai.ir/v1\"):\n",
    "        super().__init__(api_key, base_url)\n",
    "        \n",
    "    def _identify_topics(self, sentences: List[str]) -> List[str]:\n",
    "        \"\"\"Identify topics for sentences using LLM.\"\"\"\n",
    "        if not sentences:\n",
    "            return []\n",
    "            \n",
    "        # Process in batches to avoid token limits\n",
    "        topics = []\n",
    "        batch_size = 10\n",
    "        \n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i:i+batch_size]\n",
    "            batch_text = \"\\n\".join([f\"{idx+1}. {sent}\" for idx, sent in enumerate(batch)])\n",
    "            \n",
    "            prompt = f\"\"\"Analyze the following sentences and assign a brief topic/theme to each (1-3 words max).\n",
    "\n",
    "Sentences:\n",
    "{batch_text}\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\"topics\": [\"topic1\", \"topic2\", ...]}}\"\"\"\n",
    "\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CLASSIFICATION_MODEL_NAME,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=200,\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                \n",
    "                result = json.loads(response.choices[0].message.content.strip())\n",
    "                topics.extend(result.get(\"topics\", [\"general\"] * len(batch)))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error identifying topics: {e}\")\n",
    "                topics.extend([\"general\"] * len(batch))\n",
    "        \n",
    "        return topics\n",
    "\n",
    "    def _find_topic_boundaries(self, topics: List[str]) -> List[int]:\n",
    "        \"\"\"Find boundaries where topics change.\"\"\"\n",
    "        boundaries = []\n",
    "        for i in range(len(topics) - 1):\n",
    "            if topics[i] != topics[i + 1]:\n",
    "                boundaries.append(i + 1)\n",
    "        return boundaries\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    processor = SemanticDocumentProcessor(api_key=\"YOUR_API_KEY\")\n",
    "    processor.process(\n",
    "        docx_path=\"/content/3.docx\",\n",
    "        output_filename=\"semantic_chunks_new.json\",\n",
    "        max_tokens=256,\n",
    "        similarity_threshold=0.6  # Adjust for more/less strict semantic boundaries\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
