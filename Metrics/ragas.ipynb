{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nAPI_KEY = user_secrets.get_secret(\"avalai_api\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install -q openai ragas langchain_openai nltk rouge ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Json of AgenticRAG_openai_openai\n","metadata":{}},{"cell_type":"code","source":"!gdown --id \"17XpMWVjsHobPSQkqdCD2inql4HN9d92C\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# from datasets import Dataset\nfrom openai import OpenAI\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    Faithfulness, \n    AnswerRelevancy, \n)\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\nfrom langchain_openai import ChatOpenAI\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge import Rouge\n\n# --- RAGAS Evaluation ---\n\n# AvalAI credentials\n# API_KEY = \"aa-\"\nBASE_URL = \"https://api.avalai.ir/v1\"\n\n# Initialize AvalAI LLM\ndef get_eval_llm():\n    \"\"\"Initializes and returns the Language Model for evaluation.\"\"\"\n    return LangchainLLMWrapper(\n        ChatOpenAI(\n            base_url=BASE_URL,\n            model=\"gpt-4o-mini\",\n            api_key=API_KEY\n        )\n    )\n\nclient = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n\ndef create_embedding(text):\n    \"\"\"Creates an embedding for the given text using the specified model.\"\"\"\n    resp = client.embeddings.create(model=\"text-embedding-3-large\", input=text)\n    return resp.data[0].embedding\n\nclass CustomEmbeddingModel:\n    \"\"\"A custom embedding model wrapper for RAGAS.\"\"\"\n    def embed_query(self, text: str):\n        \"\"\"Embeds a single query.\"\"\"\n        return create_embedding(text)\n    def embed_documents(self, texts: list[str]):\n        \"\"\"Embeds a list of documents.\"\"\"\n        return [create_embedding(t) for t in texts]\n\ndef get_eval_emb():\n    \"\"\"Initializes and returns the embedding model for evaluation.\"\"\"\n    return LangchainEmbeddingsWrapper(CustomEmbeddingModel())\n\ndef evaluate_ragas(user_input, retrieved_contexts, generated_answer, reference_answer=None):\n    \"\"\"\n    Evaluates the RAG pipeline using the Ragas framework.\n    The reference_answer is optional. If not provided, metrics requiring it will not be run.\n\n    Args:\n        user_input (str): The user's query.\n        retrieved_contexts (list[str]): The list of retrieved document contexts.\n        generated_answer (str): The answer generated by the RAG model.\n        reference_answer (str, optional): The ground truth or reference answer. Defaults to None.\n\n    Returns:\n        dict: A dictionary containing the evaluation scores.\n    \"\"\"\n    data = {\n        \"question\": [user_input],\n        \"contexts\": [retrieved_contexts],\n        \"answer\": [generated_answer],\n    }\n    if reference_answer:\n        data[\"ground_truth\"] = [reference_answer]\n\n    ds = Dataset.from_dict(data)\n\n    llm = get_eval_llm()\n    emb = get_eval_emb()\n\n    metrics = [\n        Faithfulness(llm=llm),\n        AnswerRelevancy(llm=llm, embeddings=emb),\n    ]\n\n    result = evaluate(dataset=ds, metrics=metrics, llm=llm, embeddings=emb, show_progress=False)\n    return result.to_pandas().iloc[0].to_dict()\n\n# --- BLEU Score Evaluation ---\n\ndef evaluate_bleu(reference, candidate):\n    \"\"\"\n    Calculates the BLEU score for a candidate sentence against a reference.\n\n    Args:\n        reference (list[str]): A list of reference sentences.\n        candidate (str): The candidate sentence to evaluate.\n\n    Returns:\n        float: The BLEU score.\n    \"\"\"\n    reference_tokens = [ref.split() for ref in reference]\n    candidate_tokens = candidate.split()\n    chencherry = SmoothingFunction()\n    return sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=chencherry.method1)\n\n# --- ROUGE Score Evaluation ---\n\ndef evaluate_rouge(reference, candidate):\n    \"\"\"\n    Calculates the ROUGE score for a candidate sentence against a reference.\n\n    Args:\n        reference (str): The reference sentence.\n        candidate (str): The candidate sentence to evaluate.\n\n    Returns:\n        dict: A dictionary containing ROUGE scores (rouge-1, rouge-2, rouge-l).\n    \"\"\"\n    rouge = Rouge()\n    scores = rouge.get_scores(candidate, reference)\n    return scores[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluate_dataset.py\nimport json\nimport pathlib\nfrom statistics import mean\n\nimport pandas as pd\n\n# ---- import your metrics helpers ----\n# from my_metrics import (\n#     evaluate_ragas,\n#     evaluate_bleu,\n#     evaluate_rouge,\n# )\n\n# ---------- utility helpers ----------\ndef load_jsonl(path: str) -> list[dict]:\n    \"\"\"Reads the dataset (one JSON object per line).\"\"\"\n    with open(path, \"r\", encoding=\"utf‑8\") as f:\n        return [json.loads(line) for line in f]\n\ndef read_context(path: str) -> str:\n    \"\"\"Reads the document that was retrieved for the question.\"\"\"\n    with open(path, \"r\", encoding=\"utf‑8\") as f:\n        return f.read()\n\n# ------------- main loop -------------\ndef evaluate_dataset(json_path: str) -> pd.DataFrame:\n    rows = load_jsonl(json_path)\n    results = []\n\n    for i, row in enumerate(rows, start=1):\n        q          = row[\"question\"]\n        gen_ans    = row[\"answer\"]                       # model output\n        ref_ans    = row.get(\"reference_answer\")         # may be None\n        ctx_text   = read_context(row[\"file\"])\n\n        # -- RAGAS (faithfulness & answer‑relevancy) --\n        ragas_scores = evaluate_ragas(\n            user_input=q,\n            retrieved_contexts=[ctx_text],\n            generated_answer=gen_ans,\n            reference_answer=ref_ans,                    # OK if None\n        )\n\n        # -- BLEU & ROUGE (only if ground truth provided) --\n        if ref_ans:\n            bleu  = evaluate_bleu([ref_ans], gen_ans)\n            rouge = evaluate_rouge(ref_ans, gen_ans)\n        else:\n            bleu, rouge = None, {}\n\n        # ---- store everything ----\n        results.append(\n            {\n                \"id\": i,\n                \"question\": q,\n                \"file\": row[\"file\"],\n                \"generated_answer\": gen_ans,\n                \"reference_answer\": ref_ans,\n                \"faithfulness\": ragas_scores[\"faithfulness\"],\n                \"answer_relevancy\": ragas_scores[\"answer_relevancy\"],\n                \"bleu\": bleu,\n                **{f\"rouge_{k}\": v[\"f\"] for k, v in rouge.items()} if rouge else {},\n            }\n        )\n\n    df = pd.DataFrame(results)\n    return df\n\n\nif __name__ == \"__main__\":\n    DATA_PATH = \"data/eval_set.jsonl\"        # <‑‑ change to your file\n    OUT_PATH  = \"data/eval_results.csv\"\n\n    df = evaluate_dataset(DATA_PATH)\n    print(\"\\n=== Per‑row scores ===\")\n    print(df.to_markdown(index=False))\n\n    # ---- corpus‑level means ----\n    numeric_cols = [\"faithfulness\", \"answer_relevancy\", \"bleu\",\n                    \"rouge_1\", \"rouge_2\", \"rouge_l\"]\n    corpus_avgs = {c: mean(df[c].dropna()) for c in numeric_cols if c in df}\n    print(\"\\n=== Averages ===\")\n    for k, v in corpus_avgs.items():\n        print(f\"{k:>16}: {v:.4f}\")\n\n    # ---- save if you like ----\n    pathlib.Path(OUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n    df.to_csv(OUT_PATH, index=False)\n    print(f\"\\nDetailed results written to {OUT_PATH}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T19:36:02.451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}