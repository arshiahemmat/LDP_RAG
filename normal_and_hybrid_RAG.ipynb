{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade faiss-cpu PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MYyNNGs3duX",
        "outputId": "8eeb83e6-25f1-4883-e6dc-e42e8491999e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ZHaDQP_XvVLa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import fitz\n",
        "import faiss\n",
        "import pickle\n",
        "import numpy as np\n",
        "from typing import List\n",
        "# from dotenv import load_dotenv\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.api_core.exceptions import InternalServerError\n",
        "\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('my_api')"
      ],
      "metadata": {
        "id": "Da152x6r3iJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfXl9IrP4lWm",
        "outputId": "949ccc90-c47f-4cf5-cf9c-a4c13754be5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0skuDwvVLn"
      },
      "source": [
        "###check supported Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "-IYXjlTH395c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "XlFa20HDvVLu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "dc9e8d40-063a-4c4e-afda-68f78fe5e11c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name: models/embedding-gecko-001\n",
            "Supported methods: ['embedText', 'countTextTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-1.0-pro-vision-latest\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-pro-vision\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-pro-latest\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-pro-001\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-pro-002\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-pro\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-flash-latest\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-flash-001\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-flash-001-tuning\n",
            "Supported methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-flash\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-flash-002\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-flash-8b\n",
            "Supported methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-flash-8b-001\n",
            "Supported methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-flash-8b-latest\n",
            "Supported methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-flash-8b-exp-0827\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-1.5-flash-8b-exp-0924\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-pro-exp-03-25\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-pro-preview-03-25\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-preview-04-17\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-preview-04-17-thinking\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-pro-preview-05-06\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-exp\n",
            "Supported methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-001\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-lite-001\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-lite\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-lite-preview-02-05\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-lite-preview\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-pro-exp\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-pro-exp-02-05\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-exp-1206\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-thinking-exp-01-21\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-thinking-exp\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-thinking-exp-1219\n",
            "Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/learnlm-2.0-flash-experimental\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemma-3-1b-it\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemma-3-4b-it\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemma-3-12b-it\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemma-3-27b-it\n",
            "Supported methods: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/embedding-001\n",
            "Supported methods: ['embedContent']\n",
            "--------------------\n",
            "Model name: models/text-embedding-004\n",
            "Supported methods: ['embedContent']\n",
            "--------------------\n",
            "Model name: models/gemini-embedding-exp-03-07\n",
            "Supported methods: ['embedContent', 'countTextTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-embedding-exp\n",
            "Supported methods: ['embedContent', 'countTextTokens']\n",
            "--------------------\n",
            "Model name: models/aqa\n",
            "Supported methods: ['generateAnswer']\n",
            "--------------------\n",
            "Model name: models/imagen-3.0-generate-002\n",
            "Supported methods: ['predict']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-live-001\n",
            "Supported methods: ['bidiGenerateContent', 'countTokens']\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "for model_info in genai.list_models():\n",
        "    print(f\"Model name: {model_info.name}\")\n",
        "    print(f\"Supported methods: {model_info.supported_generation_methods}\")\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ErE8MKkFvVLy"
      },
      "outputs": [],
      "source": [
        "Instruction_prompt= \"\"\"  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "_kPiULvhvVL5"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import fitz  # PyMuPDF\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "# Optional: wrapper for Gemini\n",
        "class GeminiLLM(LLM):\n",
        "    def __init__(self, api_key: str, model_name: str, base_url: str):\n",
        "        import google.generativeai as genai\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(model_name=model_name)\n",
        "        self.base_url = base_url  # Not used in Gemini's case but added for interface consistency\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"gemini\"\n",
        "\n",
        "    def _call(self, prompt: str, stop=None, run_manager=None) -> str:\n",
        "        return self.model.generate_content(prompt).text\n",
        "\n",
        "\n",
        "class NormalRAG:\n",
        "    def __init__(self,\n",
        "                 llm_name: str = \"openai\",  # \"openai\" or \"gemini\"\n",
        "                 api_key: str = \"\",\n",
        "                 model_name: str = \"gpt-3.5-turbo\",  # or \"models/gemini-2.0-pro\"\n",
        "                 base_url: str = \"\",\n",
        "                 embed_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                 instruction_prompt: str = \"\",\n",
        "                 vectorstore_path: str = \"./faiss_index\"):\n",
        "\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key is missing.\")\n",
        "\n",
        "        self.instruction_prompt = instruction_prompt\n",
        "        self.vectorstore_path = vectorstore_path\n",
        "        self.llm_name = llm_name.lower()\n",
        "\n",
        "        # Load embedder\n",
        "        self.embedder = SentenceTransformer(embed_model_name)\n",
        "        embedding_dim = self.embedder.encode(['test']).shape[1]\n",
        "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
        "        self.documents = []\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.llm = self._load_llm(llm_name, api_key, model_name, base_url)\n",
        "\n",
        "    def _load_llm(self, llm_name: str, api_key: str, model_name: str, base_url: str) -> LLM:\n",
        "        if llm_name == \"openai\":\n",
        "            return ChatOpenAI(openai_api_key=api_key, model_name=model_name, base_url=base_url)\n",
        "        elif llm_name == \"gemini\":\n",
        "            return GeminiLLM(api_key=api_key, model_name=model_name, base_url=base_url)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported LLM provider: {llm_name}\")\n",
        "\n",
        "    def load_document(self, pdf_path: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        all_text = \"\".join([page.get_text() for page in doc])\n",
        "\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(all_text):\n",
        "            end = start + chunk_size\n",
        "            chunks.append(all_text[start:end])\n",
        "            start += chunk_size - overlap\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def add_document(self, chunks: List[str]):\n",
        "        embeddings = self.embedder.encode(chunks, batch_size=32, convert_to_numpy=True)\n",
        "        self.index.add(np.array(embeddings))\n",
        "        self.documents.extend(chunks)\n",
        "\n",
        "    def ask_question(self, query: str, top_k: int = 5) -> str:\n",
        "        query_emb = self.embedder.encode([query], convert_to_numpy=True)\n",
        "        D, I = self.index.search(np.array(query_emb), top_k)\n",
        "\n",
        "        context = \"\\n\".join([self.documents[i] for i in I[0] if i < len(self.documents)])\n",
        "        full_prompt = f\"\"\"\n",
        "            ### Instruction:\n",
        "            {self.instruction_prompt}\n",
        "\n",
        "            Use the following context to answer the question.\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Question: {query}\n",
        "            Answer:\"\"\"\n",
        "\n",
        "        for attempt in range(3):\n",
        "            try:\n",
        "                response = self.llm(full_prompt)\n",
        "                return response\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}. Retrying in 5 seconds...\")\n",
        "                time.sleep(5)\n",
        "\n",
        "        raise Exception(\"Failed to generate response after 3 retries.\")\n",
        "\n",
        "    def load_vectorstore(self):\n",
        "        if os.path.exists(self.vectorstore_path + \".index\") and os.path.exists(self.vectorstore_path + \".pkl\"):\n",
        "            self.index = faiss.read_index(self.vectorstore_path + \".index\")\n",
        "            with open(self.vectorstore_path + \".pkl\", \"rb\") as f:\n",
        "                self.documents = pickle.load(f)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_vectorstore(self):\n",
        "        faiss.write_index(self.index, self.vectorstore_path + \".index\")\n",
        "        with open(self.vectorstore_path + \".pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.documents, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fef6cfb"
      },
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "import faiss\n",
        "import fitz\n",
        "import pickle\n",
        "import os\n",
        "from typing import List, Tuple\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "from google.api_core.exceptions import InternalServerError\n",
        "import time\n",
        "\n",
        "class HybridRAG:\n",
        "    def __init__(self, api_key: str, model_name: str = \"models/gemini-2.0-flash\",\n",
        "                 embed_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                 instruction_prompt: str = Instruction_prompt,\n",
        "                 vectorstore_path: str = \"/content/drive/MyDrive/gemini_rag/hybrid_faiss_index\",\n",
        "                 bm25_corpus_path: str = \"/content/drive/MyDrive/gemini_rag/bm25_corpus.pkl\"):\n",
        "\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key is missing.\")\n",
        "\n",
        "        self.instruction_prompt = instruction_prompt\n",
        "        self.vectorstore_path = vectorstore_path\n",
        "        self.bm25_corpus_path = bm25_corpus_path\n",
        "        self.documents = []\n",
        "        self.tokenized_corpus = []\n",
        "        self.bm25 = None\n",
        "\n",
        "        # Setup Gemini\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(model_name=model_name)\n",
        "\n",
        "        # Setup Embedder\n",
        "        self.embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "        # Setup FAISS index\n",
        "        embedding_dim = self.embedder.encode(['test']).shape[1]\n",
        "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "\n",
        "    def load_document(self, pdf_path: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        all_text = \"\"\n",
        "        for page in doc:\n",
        "            all_text += page.get_text()\n",
        "\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(all_text):\n",
        "            end = start + chunk_size\n",
        "            chunks.append(all_text[start:end])\n",
        "            start += chunk_size - overlap\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def add_document(self, chunks: List[str]):\n",
        "        # Add to FAISS index\n",
        "        embeddings = self.embedder.encode(chunks, batch_size=32, convert_to_numpy=True)\n",
        "        self.index.add(np.array(embeddings))\n",
        "\n",
        "        # Add to documents list and prepare for BM25\n",
        "        self.documents.extend(chunks)\n",
        "        self.tokenized_corpus.extend([doc.split() for doc in chunks])\n",
        "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
        "\n",
        "\n",
        "    def semantic_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        query_embedding = self.embedder.encode([query], convert_to_numpy=True)\n",
        "        D, I = self.index.search(np.array(query_embedding), top_k)\n",
        "        # Return list of (document, score) tuples\n",
        "        return [(self.documents[i], float(D[0][j])) for j, i in enumerate(I[0]) if i < len(self.documents)]\n",
        "\n",
        "    def bm25_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        tokenized_query = query.split()\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "        # Return list of (document, score) tuples\n",
        "        return [(self.documents[i], float(scores[i])) for i in top_indices]\n",
        "\n",
        "    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[str]:\n",
        "        sem_results = self.semantic_search(query, top_k=top_k * 2)\n",
        "        bm25_results = self.bm25_search(query, top_k=top_k * 2)\n",
        "\n",
        "        # Combine results and rerank (simple weighted sum of scores)\n",
        "        combined_scores = {}\n",
        "        for doc, score in sem_results:\n",
        "            combined_scores[doc] = combined_scores.get(doc, 0) + (1 - alpha) * score # Lower score is better for L2\n",
        "\n",
        "        for doc, score in bm25_results:\n",
        "             # Assuming higher score is better for BM25, adjust if needed\n",
        "             # Normalize BM25 scores - simple max normalization\n",
        "            max_bm25_score = max([s for _, s in bm25_results]) if bm25_results else 1\n",
        "            normalized_bm25_score = score / max_bm25_score if max_bm25_score > 0 else 0\n",
        "\n",
        "            # For BM25, higher score is better, so use alpha * score.\n",
        "            # To combine with L2 (where lower is better), we need a consistent metric.\n",
        "            # Let's convert BM25 to a \"distance\" or \"dissimilarity\" for consistency with L2.\n",
        "            # One way is to use 1 / (1 + score) or similar.\n",
        "            # However, a simpler approach for combining is to rank and then combine ranks (Reciprocal Rank Fusion)\n",
        "            # Let's stick to a weighted sum, but be mindful of score directions.\n",
        "            # Assuming we want to rank documents such that a lower final score is better:\n",
        "            # For semantic (L2), lower is better. (1-alpha) * L2_score\n",
        "            # For BM25, higher is better. We need to invert this.\n",
        "            # Let's use Reciprocal Rank Fusion (RRF) as it's generally more robust.\n",
        "\n",
        "            # RRF Approach:\n",
        "            # Assign ranks based on individual scores, then sum reciprocal ranks.\n",
        "            pass # RRF implementation is more complex and requires re-ranking after initial search\n",
        "\n",
        "        # Simpler weighted sum approach (requires score normalization or understanding score scales)\n",
        "        # Let's assume for simplicity we normalize scores between 0 and 1 and higher is better for both after normalization\n",
        "        # This requires re-evaluating the scoring logic in semantic_search and bm25_search to return similarity scores (higher is better)\n",
        "\n",
        "        # Let's refine semantic_search and bm25_search to return similarity scores (higher is better)\n",
        "        # For L2, we can use 1 / (1 + distance).\n",
        "        # For BM25, the scores are already a form of similarity.\n",
        "\n",
        "        # Let's retry the weighted sum with normalized scores.\n",
        "        sem_results_normalized = {doc: 1 / (1 + score) for doc, score in sem_results} # Higher is better\n",
        "        max_bm25_score = max([s for _, s in bm25_results]) if bm25_results else 1\n",
        "        bm25_results_normalized = {doc: score / max_bm25_score if max_bm25_score > 0 else 0 for doc, score in bm25_results} # Higher is better\n",
        "\n",
        "        combined_scores = {}\n",
        "        for doc, score in sem_results_normalized.items():\n",
        "            combined_scores[doc] = combined_scores.get(doc, 0) + (1 - alpha) * score\n",
        "\n",
        "        for doc, score in bm25_results_normalized.items():\n",
        "            combined_scores[doc] = combined_scores.get(doc, 0) + alpha * score\n",
        "\n",
        "        # Sort by combined score (descending) and get top_k documents\n",
        "        sorted_results = sorted(combined_scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
        "\n",
        "        return [doc for doc, score in sorted_results]\n",
        "\n",
        "\n",
        "    def ask_question(self, query: str, challange: str = \"\", top_k: int = 5) -> str:\n",
        "        # Use hybrid search to get relevant context\n",
        "        context_docs = self.hybrid_search(query, top_k=top_k)\n",
        "        context = \"\\n\".join(context_docs)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "            ### instruction prompt : (explanation : this text is your guideline don't mention it on response)\n",
        "            {self.instruction_prompt}\n",
        "\n",
        "            Use the following context to answer the question.\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Question: {query}\n",
        "            Answer:\"\"\"\n",
        "\n",
        "        for attempt in range(3):\n",
        "            try:\n",
        "                response = self.model.generate_content(prompt)\n",
        "                return response.text\n",
        "            except InternalServerError as e:\n",
        "                print(f\"Error: {e}. Retrying in 5 seconds...\")\n",
        "                time.sleep(5)\n",
        "\n",
        "        raise Exception(\"Failed to generate after 3 retries.\")\n",
        "\n",
        "    def load_vectorstore(self):\n",
        "        if os.path.exists(self.vectorstore_path + \".index\") and os.path.exists(self.vectorstore_path + \".pkl\") and os.path.exists(self.bm25_corpus_path):\n",
        "            self.index = faiss.read_index(self.vectorstore_path + \".index\")\n",
        "            with open(self.vectorstore_path + \".pkl\", \"rb\") as f:\n",
        "                self.documents = pickle.load(f)\n",
        "            with open(self.bm25_corpus_path, \"rb\") as f:\n",
        "                self.tokenized_corpus = pickle.load(f)\n",
        "                self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_vectorstore(self):\n",
        "        faiss.write_index(self.index, self.vectorstore_path + \".index\")\n",
        "        with open(self.vectorstore_path + \".pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.documents, f)\n",
        "        with open(self.bm25_corpus_path, \"wb\") as f:\n",
        "            pickle.dump(self.tokenized_corpus, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Zk7hN9xhvVL-"
      },
      "outputs": [],
      "source": [
        "def load_and_add_documents(rag, pdf_files: list):\n",
        "    for file in pdf_files:\n",
        "        chunks = rag.load_document(file)\n",
        "        rag.add_document(chunks)\n",
        "    print(f\"✅ Loaded and embedded {len(pdf_files)} document(s).\")\n",
        "\n",
        "def chat_loop(rag):\n",
        "    print(\"type exit\")\n",
        "    exit_keywords = ['exit']\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input('\\nUser: ').strip()\n",
        "            if user_input.lower() in [e.lower() for e in exit_keywords]:\n",
        "                print('\\n')\n",
        "                break\n",
        "            challenges = rag.generate_challenges(user_input)\n",
        "            # give choosen challange to model\n",
        "            response = rag.ask_question(user_input, challange= \"\")\n",
        "            print('Bot:', response)\n",
        "            print('-' * 50)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ error: {e}\")\n",
        "            continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ay3udvh_vVMB"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Run RAG with specified parameters.\")\n",
        "    parser.add_argument('--data-name', type=str, default='all', help='Name of the data to use (e.g., all).')\n",
        "    parser.add_argument('--emb-model', type=str, default=\"sentence-transformers/all-MiniLM-L6-v2\", help='Name of the embedding model to use.')\n",
        "    parser.add_argument('--RAG-type', type=str, default='Normal', help='Type of RAG to use (e.g., Normal, Hybrid).')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.RAG_type.lower() == 'normal':\n",
        "        rag = NormalRAG(api_key=api_key, embed_model_name=args.emb_model)\n",
        "    elif args.RAG_type.lower() == 'hybrid':\n",
        "        rag = HybridRAG(api_key=api_key, embed_model_name=args.emb_model)\n",
        "    else:\n",
        "        print(f\"Unknown RAG type: {args.RAG_type}. Please choose 'Normal' or 'Hybrid'.\")\n",
        "        return\n",
        "\n",
        "    if rag.load_vectorstore():\n",
        "        print(f\"📦 Loaded {args.RAG_type} vectorstore from Google Drive.\")\n",
        "    else:\n",
        "        print(f\"ℹ️ No saved {args.RAG_type} vectorstore found. Embedding from scratch.\")\n",
        "        # This part might also depend on data-name, you'll need to add logic here\n",
        "        pdf_files = [\"drive/MyDrive/gemini_rag/rag_file1.pdf\"]\n",
        "        load_and_add_documents(rag, pdf_files)\n",
        "        rag.save_vectorstore()\n",
        "        print(f\"✅ {args.RAG_type} Vectorstore saved to Google Drive.\")\n",
        "\n",
        "    chat_loop(rag)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1zHcxIz5Q8k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}